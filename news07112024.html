<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="utf-8">
      <title>News</title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="description" content="Neurobiology of Language Lab, The Chinese University of Hong Kong">
      <meta name="author" content="">
      <!-- Le styles -->
      <link href="css/bootstrap.min.css" rel="stylesheet">
      <link href="css/bootstrap-responsive.min.css" rel="stylesheet">
      <link href="css/theme.css" rel="stylesheet">
   </head>
   <body>
      <div class="container">
         <header class="jumbotron subhead" id="overview">
            <p class="lead"> The Chinese University of Hong Kong </p>
            <h1>Neurobiology of Language Lab (NLL)</h1>
         </header>
         <div class="masthead">
            <div class="navbar">
               <div class="navbar-inner">
                  <div class="container">
                     <ul class="nav">
                        <li><a href="index.html">Home</a></li>
                        <li><a href="people.html">People</a></li>
                        <li><a href="publication.html">Publication</a></li>
                        <li><a href="research.html">Research</a></li>
                        <li><a href="activity.html">Activity</a></li>
                        <li><a href="opening.html">Join Us</a></li>
                     </ul>
                  </div>
               </div>
            </div>
         </div>
         
         <div class="container-fluid">
            <div class="row-fluid marketing">
               <div class="span12">
                  <h4>One paper accepted by SNL 2024. Congratulations to Chen Hong!</h4>
                  <hr>  
                  <strong>Abstract</strong>          
                  <p>
                    Language allows us to communicate our thoughts and feelings to others. Verbal communication conveys different levels of linguistic information through sounds. The speaker translates thoughts into sounds, while the listener hears the sounds and interprets them into meaningful content. These communication processes are proposed to be supported by synchronized neural activities between speakers and listeners across our speech production and perception systems. However, we are still far from understanding what speech content drives the speaker-listener neural synchronization and when and how the informational content transforms from the speaker’s production system to the listener’s perception system. To address these questions, we conducted a pseudo-hyperscanning magnetoencephalography (MEG) experiment with a story-telling paradigm and applied an encoding modeling approach to estimate the contribution of acoustic and sublexical linguistic contents (segmental and supra-segmental units) to the speaker-listener neural synchronizations. Our MEG experiment involved recording the neural signals of a speaker while the speaker naturally re-told eight stories (7 minutes per story) after reading scripts. We then collected the MEG responses of 22 listeners as they listened to the recorded stories. We estimated the cortical source activities across regions related to speech production and perception for both the speaker and listeners using dynamic statistical parametric mapping (dSPM). We used the temporal response function (TRF) approach to estimate to what extent the neural signals of the speaker predict the neural signals of the listeners to uncover shared neural variances (i.e., neural couplings) and the associated time lags between the speaker and listeners’ neural responses. We further estimated the extent to which the acoustic and sublexical linguistic features of the speech can account for these shared neural variances. We found significant speaker-listener neural couplings in a wide spread of regions, including auditory temporal (STG/HG), middle temporal (MTG/ITG), parietal (SMG/IPL), frontal (IFG), and central gyri (Pre/PostCG). The listeners’ auditory regions are best coupled with the speaker’s overall neural responses, with a degrading pattern from auditory to higher-level fronto-temporo-parietal regions (STG/HG, r = 0.016; MTG/ITG, r = 0.015, SMG/IPL, r = 0.012; IFG, r = 0.011; Pre/PostCG, r = 0.009); whereas the peak time differences in coupling show an opposite pattern (IFG = ~340ms; MTG/ITG = ~ 260ms; Pre/PostCG = ~260ms; STG/HG = ~260ms; SMG/IPL = ~210ms). Importantly, we found that both acoustic and linguistic features contributed to the neural couplings. Specifically, all features showed significant unique contributions, where the tone category and pitch height significantly outperformed other features in explaining neural couplings. We also identified weak but significant common contributions between acoustic and linguistic features as well as common contributions among segmental and supra-segmental features, but they differed in temporal latency. These findings not only reveal synchronized neural responses between speaker and listener but also demonstrate how linguistic features in assemble drive neural synchronizations. These findings shed light on revealing the common linguistic space created by interlocutors and the shared mechanisms of production-perception systems in representing linguistic information, highlighting the interconnected nature of language processing between individuals engaged in communication.
                  </p>
                  <p>
                    <strong>Topic Areas</strong>: Language Production, Computational Approaches
                  </p>
                  <img src="images/poster_07112024.png" class="carousel-image" alt=""> 
               </div>
            </div>
         </div>
         
      </div>

      <!-- Le javascript
         ================================================== -->
      <!-- Placed at the end of the document so the pages load faster -->
      <script src="js/jquery-1.9.1.min.js"></script>
      <script src="js/bootstrap.min.js"></script>
      <script>
         $(document).ready(function() {
             $(document.body).scrollspy({
                 target: "#navparent"
             });
         });
         
      </script>
   </body>
</html>